// Copyright(C) Facebook, Inc. and its affiliates.
use anyhow::{Context, Result};
use clap::{crate_name, crate_version, App, AppSettings, ArgMatches, SubCommand};
use config::Export as _;
use config::Import as _;
use config::{Committee, KeyPair, Parameters, WorkerId};
use consensus::Consensus;
use env_logger::Env;
use libp2p::Multiaddr;
use log::info;
use primary::{Certificate, Primary};
use store::Store;
use tokio::sync::mpsc::{channel, Receiver};
use worker::{Worker, WorkerMessage};
// Th√™m c√°c use statements c·∫ßn thi·∫øt
use bytes::{BufMut, Bytes, BytesMut};
use prost::Message;
use tokio::io::AsyncWriteExt;
use tokio::net::UnixStream;
use tokio::time::{sleep, Duration};

// Import P2P modules
use libp2p::{gossipsub, PeerId};
use p2p::{
    event_loop,
    functions::{create_derived_keypair},
    types::{get_primary_topic, get_worker_topic, P2pMessage, PrimaryMessage1, WorkerMessage1},
    ReqResCommand, ReqResEvent,
};

// Th√™m module ƒë·ªÉ import c√°c struct ƒë∆∞·ª£c t·∫°o b·ªüi prost
pub mod comm {
    include!(concat!(env!("OUT_DIR"), "/comm.rs"));
}

/// The default channel capacity.
pub const CHANNEL_CAPACITY: usize = 1_000;

#[tokio::main]
async fn main() -> Result<()> {
    let matches = App::new(crate_name!()) //// ƒê·ªãnh nghƒ©a c√°c l·ªánh v√† tham s·ªë
        .version(crate_version!())
        .about("A research implementation of Narwhal and Tusk.")
        .args_from_usage("-v... 'Sets the level of verbosity'")
        .subcommand(
            SubCommand::with_name("generate_keys")
                .about("Print a fresh key pair to file")
                .args_from_usage("--filename=<FILE> 'The file where to print the new key pair'"),
        )
        .subcommand(
            SubCommand::with_name("run")
                .about("Run a node")
                .args_from_usage("--keys=<FILE> 'The file containing the node keys'")
                .args_from_usage("--committee=<FILE> 'The file containing committee information'")
                .args_from_usage("--parameters=[FILE] 'The file containing the node parameters'")
                .args_from_usage("--store=<PATH> 'The path where to create the data store'")
                .subcommand(SubCommand::with_name("primary").about("Run a single primary"))
                .subcommand(
                    SubCommand::with_name("worker")
                        .about("Run a single worker")
                        .args_from_usage("--id=<INT> 'The worker id'"),
                )
                .setting(AppSettings::SubcommandRequiredElseHelp),
        )
        .setting(AppSettings::SubcommandRequiredElseHelp)
        .get_matches();
    // ... Thi·∫øt l·∫≠p logging ...
    let log_level = match matches.occurrences_of("v") {
        0 => "error",
        1 => "warn",
        2 => "info",
        3 => "debug",
        _ => "trace",
    };
    let mut logger = env_logger::Builder::from_env(Env::default().default_filter_or(log_level));
    #[cfg(feature = "benchmark")]
    logger.format_timestamp_millis();
    logger.init();

    match matches.subcommand() {
        ("generate_keys", Some(sub_matches)) => KeyPair::new()
            .export(sub_matches.value_of("filename").unwrap())
            .context("Failed to generate key pair")?,
        ("run", Some(sub_matches)) => run(sub_matches).await?,
        _ => unreachable!(),
    }
    Ok(())
}

// Runs either a worker or a primary.
async fn run(matches: &ArgMatches<'_>) -> Result<()> {
    // 1. ƒê·ªçc c√°c ƒë∆∞·ªùng d·∫´n file t·ª´ tham s·ªë
    let key_file = matches.value_of("keys").unwrap();
    let committee_file = matches.value_of("committee").unwrap();
    let parameters_file = matches.value_of("parameters");
    let store_path = matches.value_of("store").unwrap();
    // 2. T·∫£i c·∫•u h√¨nh t·ª´ c√°c file JSON
    // Read the committee and node's keypair from file.
    let keypair = KeyPair::import(key_file).context("Failed to load the node's keypair")?;

    let committee =
        Committee::import(committee_file).context("Failed to load the committee information")?;

    // Load default parameters if none are specified.
    let parameters = match parameters_file {
        Some(filename) => {
            Parameters::import(filename).context("Failed to load the node's parameters")?
        }
        None => Parameters::default(),
    };
    // Make the data store.
    let store = Store::new(store_path).context("Failed to create a store")?;

    // Channels the sequence of certificates.
    let (tx_output, rx_output) = channel(CHANNEL_CAPACITY);

    let role: &str;
    // 2. X√ÅC ƒê·ªäNH VAI TR√í V√Ä T·∫†O P2P KEYPAIR DUY NH·∫§T
    // Logic n√†y ƒë∆∞·ª£c chuy·ªÉn l√™n tr∆∞·ªõc ƒë·ªÉ quy·∫øt ƒë·ªãnh component_id
    let (p2p_keypair, _) = match matches.subcommand() {
        ("primary", _) => {
            role = "Primary";
            info!("Running as a PRIMARY");
            // S·ª≠ d·ª•ng m·ªôt ID ƒë·∫∑c bi·ªát cho Primary ƒë·ªÉ ƒë·∫£m b·∫£o n√≥ kh√°c v·ªõi m·ªçi Worker
            let key = create_derived_keypair(&keypair.name, u32::MAX);
            (key, "Primary")
        }
        ("worker", Some(sub_matches)) => {
            role = "Worker";
            let id = sub_matches.value_of("id").unwrap().parse::<WorkerId>()?;
            info!("Running as a WORKER with id {}", id);
            // S·ª≠ d·ª•ng ch√≠nh WorkerId ƒë·ªÉ t·∫°o keypair duy nh·∫•t
            let key = create_derived_keypair(&keypair.name, id);
            (key, "Worker")
        }
        _ => unreachable!(),
    }; 

    let local_peer_id = p2p_keypair.public().to_peer_id();
    info!("üìå P2P Peer ID for {}: {}", role, local_peer_id);

    // T·∫°o Swarm
    let mut swarm = p2p::create_p2p_swarm(p2p_keypair, local_peer_id)
        .await
        .expect("Failed to create P2P swarm");


    // L·∫Øng nghe tr√™n m·ªôt port duy nh·∫•t (logic t√¨m port tr·ªëng c·ªßa b·∫°n r·∫•t t·ªët)
    let mut p2p_port = 9000; // Port kh·ªüi ƒë·∫ßu
    for _ in 0..15 {
        let listen_addr: Multiaddr = format!("/ip4/0.0.0.0/udp/{}/quic-v1", p2p_port).parse()?;
        match swarm.listen_on(listen_addr) {
            Ok(_) => {
                info!("‚úÖ P2P listening on port {}", p2p_port);
                break;
            }
            Err(e) => {
                info!("‚ö†Ô∏è Port {} in use, trying next. Error: {}", p2p_port, e);
                p2p_port += 1;
            }
        }
    }
    // ƒêƒÉng k√Ω (subscribe) v√†o c√°c topic ph√π h·ª£p v·ªõi vai tr√≤ c·ªßa node
    let primary_topic = get_primary_topic();
    let worker_topic = get_worker_topic();
    if matches.subcommand_matches("primary").is_some() {
        swarm.behaviour_mut().gossipsub.subscribe(&primary_topic)?;
        info!("P ___Subscribed to PRIMARY topic: {}", primary_topic);
    }
    if matches.subcommand_matches("worker").is_some() {
        swarm.behaviour_mut().gossipsub.subscribe(&worker_topic)?;
        info!("W ___Subscribed to WORKER topic: {}", worker_topic);
    }
    // --- 3. T·∫†O C√ÅC K√äNH GIAO TI·∫æP C√ì C·∫§U TR√öC ---
    // K√™nh ƒë·ªÉ logic ch√≠nh g·ª≠i message ra P2P event loop
    let (tx_to_p2p, rx_from_core) = channel(CHANNEL_CAPACITY);
    // K√™nh ƒë·ªÉ P2P event loop g·ª≠i message ƒë√£ x·ª≠ l√Ω v√†o logic Primary
    let (tx_to_primary, mut rx_for_primary) = channel(CHANNEL_CAPACITY);
    // K√™nh ƒë·ªÉ P2P event loop g·ª≠i message ƒë√£ x·ª≠ l√Ω v√†o logic Worker
    let (tx_to_worker, mut rx_for_worker) = channel(CHANNEL_CAPACITY);
    
    // K√™nh cho Request-Response
    let (tx_req_res_command, rx_req_res_command) = channel(CHANNEL_CAPACITY);
    let (tx_req_res_event, mut rx_req_res_event) = channel(CHANNEL_CAPACITY);
    
    // Ch·∫°y P2P event loop trong m·ªôt task ri√™ng
    tokio::spawn(async move {
        event_loop::run_p2p_event_loop(
            swarm, 
            rx_from_core, 
            tx_to_primary, 
            tx_to_worker,
            rx_req_res_command,
            tx_req_res_event,
        ).await;
    });

    // --- K·∫æT TH√öC KH·ªûI T·∫†O P2P ---
    
    // --- 4A. DEMO GOSSIPSUB (Hello messages) ---
    let tx_p2p_clone = tx_to_p2p.clone();
    let key_name_clone = keypair.name.clone();
    tokio::spawn(async move {
        use tokio::time::{sleep, Duration};
        sleep(Duration::from_secs(5)).await; // ƒê·ª£i m·∫°ng ·ªïn ƒë·ªãnh
        let mut counter: u64 = 0;
        loop {
           counter += 1;
           
           // Ch·ªâ g·ª≠i Hello message qua gossipsub
           let hello_msg = format!(
               "Hello from {} {}: [Seq: {}]",
               role, key_name_clone, counter
           );

           let (topic, p2p_message) = if role == "Primary" {
               (get_primary_topic(), P2pMessage::Primary(PrimaryMessage1::Hello(hello_msg)))
           } else {
               (get_worker_topic(), P2pMessage::Worker(WorkerMessage1::Hello(hello_msg)))
           };
           
        //    log::info!("üì§ [GOSSIPSUB] Sending Hello message #{}", counter);
        //    if tx_p2p_clone.send((topic, p2p_message)).await.is_err() {
        //        log::warn!("Core logic channel closed, stopping demo sender.");
        //        break;
        //    }
           
           sleep(Duration::from_secs(10)).await;
        }
    });
    
    // --- 4B. DEMO REQUEST-RESPONSE (V·ªõi ACK tracking) ---
    let tx_req_res_cmd_clone = tx_req_res_command.clone();
    let key_name_clone2 = keypair.name.clone();
    let local_peer_id_clone = local_peer_id;
    tokio::spawn(async move {
        sleep(Duration::from_secs(8)).await; // ƒê·ª£i m·∫°ng ·ªïn ƒë·ªãnh v√† peer discovery
        
        // L·∫•y danh s√°ch peers ƒë√£ discovered (s·∫Ω ƒë∆∞·ª£c populate b·ªüi mDNS)
        // Trong production, b·∫°n s·∫Ω track peers qua mDNS events
        let mut request_counter: u64 = 0;
        
        loop {
            request_counter += 1;
            let discovered_peer_id = local_peer_id_clone; // Gi·∫£ s·ª≠ ch√∫ng ta c√≥ m·ªôt peer kh√°c ƒë·ªÉ g·ª≠i request
            // Trong th·ª±c t·∫ø, b·∫°n s·∫Ω maintain m·ªôt list peers t·ª´ mDNS/Kad
            log::info!("üì§ [REQ-RES DEMO] Would send Request #{} if peers available", request_counter);
            log::info!("    üí° To demo: Run another node, it will auto-discover via mDNS");
            log::info!("    üí° Requests will be sent automatically to discovered peers");
            
          //  TODO: Khi c√≥ peer, g·ª≠i request nh∆∞ sau:
            let data = format!("Request data from {}", key_name_clone2);
            if let Err(e) = tx_req_res_cmd_clone.send(ReqResCommand::SendRequest {
                request_id: request_counter,
                target_peer: discovered_peer_id,
                data: data.into_bytes(),
            }).await {
                log::warn!("Failed to send ReqResCommand: {}", e);
            }
            
            sleep(Duration::from_secs(15)).await;
        }
    });
    
    // --- 4C. Task l·∫Øng nghe Request-Response Events (ACKs) ---
    tokio::spawn(async move {
        use std::collections::HashMap;
        let mut pending_requests: HashMap<u64, tokio::time::Instant> = HashMap::new();
        
        info!("üëÇ [REQ-RES] Event listener started");
        
        while let Some(event) = rx_req_res_event.recv().await {
            match event {
                ReqResEvent::ResponseReceived { request_id, from_peer, success, message } => {
                    if let Some(sent_time) = pending_requests.remove(&request_id) {
                        let latency = sent_time.elapsed();
                        info!("‚úÖ [REQ-RES] ACK received for Request #{}", request_id);
                        info!("    ‚îú‚îÄ From: {}", from_peer);
                        info!("    ‚îú‚îÄ Success: {}", success);
                        info!("    ‚îú‚îÄ Message: '{}'", message);
                        info!("    ‚îî‚îÄ Latency: {:?}", latency);
                    }
                }
                ReqResEvent::RequestFailed { request_id, to_peer, error } => {
                    pending_requests.remove(&request_id);
                    log::warn!("‚ùå [REQ-RES] Request #{} failed to {}: {}", request_id, to_peer, error);
                }
                ReqResEvent::RequestReceived { request_id, from_peer, data } => {
                    info!("üì® [REQ-RES] Received Request #{} from {}", request_id, from_peer);
                    info!("    ‚îî‚îÄ Data: {} bytes", data.len());
                    // ACK ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông g·ª≠i trong handler
                }
            }
        }
    });

    // Task l·∫Øng nghe message cho Primary
    tokio::spawn(async move {
        info!("üëÇ PRIMARY message listener task started.");
        while let Some(message) = rx_for_primary.recv().await {
            match message {
                PrimaryMessage1::Hello(msg) => {
                    info!("üéâ [PRIMARY LOGIC] Received Hello: '{}'", msg);
                }
                PrimaryMessage1::Request { request_id, data } => {
                    info!("üì® [PRIMARY LOGIC] Received Request #{}: {}", request_id, data);
                    // Logic x·ª≠ l√Ω request ·ªü ƒë√¢y (response ƒë√£ t·ª± ƒë·ªông g·ª≠i trong handler)
                }
                PrimaryMessage1::Response { request_id, data } => {
                    info!("‚úÖ [PRIMARY LOGIC] Received Response #{}: {}", request_id, data);
                    // X·ª≠ l√Ω response ·ªü ƒë√¢y
                }
                // Th√™m c√°c case x·ª≠ l√Ω cho c√°c lo·∫°i message kh√°c c·ªßa Primary
                // PrimaryMessage::Header(header_bytes) => { ... }
            }
        }
    });

    // Task l·∫Øng nghe message cho Worker
    tokio::spawn(async move {
        info!("üëÇ WORKER message listener task started.");
        while let Some(message) = rx_for_worker.recv().await {
             match message {
                WorkerMessage1::Hello(msg) => {
                     info!("üéâ [WORKER LOGIC] Received Hello: '{}'", msg);
                }
                WorkerMessage1::Request { request_id, data } => {
                    info!("üì® [WORKER LOGIC] Received Request #{}: {}", request_id, data);
                    // Logic x·ª≠ l√Ω request ·ªü ƒë√¢y (response ƒë√£ t·ª± ƒë·ªông g·ª≠i trong handler)
                }
                WorkerMessage1::Response { request_id, data } => {
                    info!("‚úÖ [WORKER LOGIC] Received Response #{}: {}", request_id, data);
                    // X·ª≠ l√Ω response ·ªü ƒë√¢y
                }
                // Th√™m c√°c case x·ª≠ l√Ω cho c√°c lo·∫°i message kh√°c c·ªßa Worker
             }
        }
    });

    // Check whether to run a primary, a worker, or an entire authority.
    match matches.subcommand() {
        // Spawn the primary and consensus core.
        // N√≥ kh·ªüi ch·∫°y c·∫£ Primary (ƒë·ªÅ xu·∫•t c√°c kh·ªëi/header) v√† Consensus (s·∫Øp x·∫øp c√°c kh·ªëi/header ƒë√≥).
        // Hai th√†nh ph·∫ßn n√†y giao ti·∫øp v·ªõi nhau qua c√°c k√™nh (channels).
        // Quan tr·ªçng nh·∫•t, k·∫øt qu·∫£ cu·ªëi c√πng c·ªßa qu√° tr√¨nh ƒë·ªìng thu·∫≠n (m·ªôt d√≤ng c√°c Certificate ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp) ƒë∆∞·ª£c g·ª≠i qua k√™nh tx_output.
        // Sau ƒë√≥, n√≥ g·ªçi h√†m analyze(rx_output, ...) ƒë·ªÉ x·ª≠ l√Ω d√≤ng Certificate n√†y.
        ("primary", _) => {
            // X√°c ƒë·ªãnh ID duy nh·∫•t cho node n√†y d·ª±a tr√™n v·ªã tr√≠ public key trong committee.
            let mut primary_keys: Vec<_> = committee.authorities.keys().cloned().collect();
            primary_keys.sort(); // S·∫Øp x·∫øp ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª± l√† nh·∫•t qu√°n tr√™n t·∫•t c·∫£ c√°c node.

            let node_id = primary_keys
                .iter()
                .position(|pk| pk == &keypair.name)
                .unwrap();
            log::info!("Node {} kh·ªüi ch·∫°y v·ªõi ID: {}", keypair.name, node_id);
            //ƒê·ªÅ xu√¢t cho Ng∆∞·ªùi ƒëi·ªÅu ph·ªëi (Consensus)
            let (tx_new_certificates, rx_new_certificates) = channel(CHANNEL_CAPACITY);
            // k√™nh nh·∫≠n ph·∫£n h·ªìi t·ª´ Ng∆∞·ªùi ƒëi·ªÅu ph·ªëi (Consensus)
            let (tx_feedback, rx_feedback) = channel(CHANNEL_CAPACITY);
            // B·∫Øt ƒë·∫ßu c√¥ng vi·ªác c·ªßa B·∫øp tr∆∞·ªüng
            Primary::spawn(
                keypair,
                committee.clone(),
                parameters.clone(),
                store.clone(), // Clone the store for the primary
                /* tx_consensus */ tx_new_certificates,
                /* rx_consensus */ rx_feedback,
            );
            // B·∫Øt ƒë·∫ßu c√¥ng vi·ªác c·ªßa Ng∆∞·ªùi ƒëi·ªÅu ph·ªëi
            Consensus::spawn(
                committee,
                parameters.gc_depth,
                store.clone(),
                rx_new_certificates,
                tx_feedback,
                tx_output,
            );
            // Giao k·∫øt qu·∫£ cu·ªëi c√πng cho ng∆∞·ªùi ph·ª•c v·ª•
            analyze(rx_output, node_id, store).await;
        }
        //Ch·ªâ ƒë∆°n gi·∫£n l√† kh·ªüi ch·∫°y m·ªôt Worker. Worker ch·ªãu tr√°ch nhi·ªám nh·∫≠n giao d·ªãch t·ª´ client v√† t·∫°o c√°c batch (l√¥) giao d·ªãch.
        // Spawn a single worker.
        ("worker", Some(sub_matches)) => {
            let id = sub_matches
                .value_of("id")
                .unwrap()
                .parse::<WorkerId>()
                .context("The worker id must be a positive integer")?;
            
            // TODO: T·∫°o peer_mapping t·ª´ committee (map PublicKey -> PeerId)
            // Hi·ªán t·∫°i d√πng HashMap r·ªóng, c·∫ßn implement peer discovery sau
            let peer_mapping = std::collections::HashMap::new();
            
            Worker::spawn(
                keypair.name, 
                id, 
                committee, 
                parameters, 
                store,
                Some(tx_req_res_command.clone()),
                None, // tx_req_res_event_to_quorum s·∫Ω ƒë∆∞·ª£c t·∫°o b√™n trong Worker
                peer_mapping,
            );
        }
        _ => unreachable!(),
    }

    // Gi·ªØ cho ch∆∞∆°ng tr√¨nh ch√≠nh kh√¥ng b·ªã tho√°t, cho ph√©p c√°c task con (primary/worker) ti·∫øp t·ª•c ch·∫°y.
    // Ch√∫ng ta t·∫°o m·ªôt k√™nh m·ªõi v√† ch·ªù m√£i m√£i ·ªü ƒë·∫ßu nh·∫≠n.
    let (_tx, mut rx) = channel::<()>(1);
    let _ = rx.recv().await;

    unreachable!();
}

//C·∫ßu n·ªëi ƒë·∫øn L·ªõp Th·ª±c thi (Executor)
/// Receives an ordered list of certificates and apply any application-specific logic.
async fn analyze(mut rx_output: Receiver<Certificate>, node_id: usize, mut store: Store) {
    // S·ª¨A L·ªñI: Th√™m `mut` v√†o ƒë√¢y
    // Helper function for varint encoding (no changes needed here)
    fn put_uvarint_to_bytes_mut(buf: &mut BytesMut, mut value: u64) {
        loop {
            if value < 0x80 {
                buf.put_u8(value as u8);
                break;
            }
            buf.put_u8(((value & 0x7F) | 0x80) as u8);
            value >>= 7;
        }
    }
    // 1. K·∫øt n·ªëi t·ªõi Executor qua Unix Socket
    let socket_path = format!("/tmp/executor{}.sock", node_id);
    log::info!(
        "[ANALYZE] Node ID {} attempting to connect to {}",
        node_id,
        socket_path
    );
    //// V√≤ng l·∫∑p k·∫øt n·ªëi l·∫°i n·∫øu th·∫•t b·∫°i
    let mut stream = loop {
        match UnixStream::connect(&socket_path).await {
            Ok(stream) => {
                log::info!(
                    "[ANALYZE] Node ID {} connected successfully to {}",
                    node_id,
                    socket_path
                );
                break stream;
            }
            Err(e) => {
                log::warn!(
                    "[ANALYZE] Node ID {}: Connection to {} failed: {}. Retrying...",
                    node_id,
                    socket_path,
                    e
                );
                tokio::time::sleep(tokio::time::Duration::from_secs(2)).await;
            }
        }
    };

    log::info!(
        "[ANALYZE] Node ID {} entering loop to wait for committed blocks.",
        node_id
    );
    // 2. V√≤ng l·∫∑p ch√≠nh: ch·ªù k·∫øt qu·∫£ t·ª´ Consensus
    while let Some(certificate) = rx_output.recv().await {
        log::info!(
            "[ANALYZE] Node ID {} RECEIVED certificate for round {} from consensus.",
            node_id,
            certificate.header.round
        );

        let mut all_transactions = Vec::new();

        for (digest, worker_id) in certificate.header.payload {
            // ƒê·ªçc batch t·ª´ store b·∫±ng digest.
            match store.read(digest.to_vec()).await {
                Ok(Some(serialized_batch_message)) => {
                    // Gi·∫£i m√£ `WorkerMessage::Batch`
                    match bincode::deserialize(&serialized_batch_message) {
                        Ok(WorkerMessage::Batch(batch)) => {
                            log::debug!(
                                "[ANALYZE] Unpacked batch {} with {} transactions for worker {}.",
                                digest,
                                batch.len(),
                                worker_id
                            );
                            // Chuy·ªÉn ƒë·ªïi m·ªói giao d·ªãch trong batch sang ƒë·ªãnh d·∫°ng protobuf.
                            for tx_data in batch {
                                all_transactions.push(comm::Transaction {
                                    // Tr∆∞·ªùng 'digest' trong protobuf gi·ªù s·∫Ω ch·ª©a to√†n b·ªô d·ªØ li·ªáu giao d·ªãch.
                                    digest: tx_data,
                                    worker_id: worker_id as u32,
                                });
                            }
                        }
                        Ok(_) => {
                            log::warn!(
                                "[ANALYZE] Digest {} did not correspond to a Batch message.",
                                digest
                            );
                        }
                        Err(e) => {
                            log::error!(
                                "[ANALYZE] Failed to deserialize message for digest {}: {}",
                                digest,
                                e
                            );
                        }
                    }
                }
                Ok(None) => {
                    log::warn!("[ANALYZE] Batch for digest {} not found in store.", digest);
                }
                Err(e) => {
                    log::error!(
                        "[ANALYZE] Failed to read batch for digest {}: {}",
                        digest,
                        e
                    );
                }
            }
        }

        // B·ªè qua vi·ªác g·ª≠i block n·∫øu kh√¥ng c√≥ giao d·ªãch n√†o ƒë∆∞·ª£c t√¨m th·∫•y
        if all_transactions.is_empty() {
            log::warn!(
                "[ANALYZE] No transactions found for certificate in round {}. Skipping.",
                certificate.header.round
            );
            continue;
        }

        let committed_block = comm::CommittedBlock {
            epoch: certificate.header.round,
            height: certificate.header.round,
            transactions: all_transactions, // S·ª≠ d·ª•ng danh s√°ch ƒë·∫ßy ƒë·ªß c√°c giao d·ªãch
        };

        let epoch_data = comm::CommittedEpochData {
            blocks: vec![committed_block],
        };

        log::debug!(
            "[ANALYZE] Node ID {} serializing data for round {}",
            node_id,
            certificate.header.round
        );
        let mut proto_buf = BytesMut::new();
        epoch_data
            .encode(&mut proto_buf)
            .expect("FATAL: Protobuf serialization failed!");

        let mut len_buf = BytesMut::new();
        put_uvarint_to_bytes_mut(&mut len_buf, proto_buf.len() as u64);

        log::info!("[ANALYZE] Node ID {} WRITING {} bytes (len) and {} bytes (data) to socket for round {}.", node_id, len_buf.len(), proto_buf.len(), certificate.header.round);

        if let Err(e) = stream.write_all(&len_buf).await {
            log::error!(
                "[ANALYZE] FATAL: Node ID {}: Failed to write length to socket: {}",
                node_id,
                e
            );
            break;
        }

        if let Err(e) = stream.write_all(&proto_buf).await {
            log::error!(
                "[ANALYZE] FATAL: Node ID {}: Failed to write payload to socket: {}",
                node_id,
                e
            );
            break;
        }

        log::info!(
            "[ANALYZE] SUCCESS: Node ID {} sent block for round {} successfully.",
            node_id,
            certificate.header.round
        );
    }

    log::warn!(
        "[ANALYZE] Node ID {} exited the receive loop. No more blocks will be processed.",
        node_id
    );
}
